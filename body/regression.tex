\chapter{Räumliche Regression}
\label{ch:regression}

Regressionsmodelle stellen die primäre Klasse zur räumlichen Modellanalyse dar. Zunächst bietet sich die Klasse 
der \emph{multiplen linearen Regression} (MLR) an. Sie dient mit $k$ unabhängigen Regressoren der Erklärung einer 
abhängigen Zielgröße (Regressand). Dies stellt eine Spezialisierung des \emph{allgemeinen linearen Modells}, 
auch \emph{multivariates lineares Regressionsmodell} dar, welches mehrere korrelierte (abhängige) Zielgrößen modelliert. Die multiple lineare Regression setzt 
aber in der Grundvariante als \emph{klassisches multiples lineares Modell} ($\mathbf{KL}$-Modell) identisch und unabhängig 
verteilte (i.i.d.) Störterme voraus. Das \emph{klassisch lineare Modell der Normalregression} ($\mathbf{KLN}$-Modell) geht zusätzlich noch von einer 
Normalverteilung der Störgrößen aus.
Da wir jedoch Heteroskedastizität und Autokorrelation berücksichtigen, 
nutzen wir in dieser Arbeit das \emph{verallgemeinerte multiple lineare Modell} ($\mathbf{VL}$-Modell) mit einer erweiterten Varianz-Kovarianzmatrix der Fehlerterme.
(Heteroskedastizität liegt z.B. vor, wenn die Fehlerterme verschiedener Zielgrößen unterschiedliche Varianzen aufweisen.)

$Y$ stellt einen Vektor aus Beobachtungswerten dar und wird im Regressionsmodell als zu erklärende, abhängige Variable modelliert 
und als Regressand, endogene Variable oder Zielgröße (engl. regressand, endogenous variable, response variable) bezeichnet.

$X$ besteht hingegen als Matrix aus insgesamt $k$ n-dimensionalen Spaltenvektoren als den erklärenden, unabhängigen Variablen, welche auch als Regressoren, exogene Variablen oder Kovariate
(engl. regressors, exogenous variables, explanatory variables, covariates) bezeichnet werden.

Die Fehlerterme bzw. Störgrößen (engl. error term, disturbance term, noise) werden mit $\varepsilon$ bezeichnet.

Die \emph{gewöhnliche Methode der kleinsten Quadrate} ($\mathbf{GKQ}$-Methode) (engl. ordinary least squares - OLS)) dient als Standardverfahren der Ausgleichsrechnung 
zur Schätzung der unbekannten Modellparameter $\beta, \sigma^{2}$ und $\rho$ im KL-Modell.
Dagegen dient die Verallgemeinerte Kleinste-Quadrate-Schätzung ($\mathbf{VKQ}$-Methode) (engl. generalized least squares - GLS) der Parametrisierung des VL-Modells.

Der Unterschied in der räumlichen Struktur zwischen stetigen Punktlokationen der Geostatistik und 
diskreten Raumeinheiten als Partition des Gesamtraumes in der Gitterdatenalyse spiegelt sich auch in den Regressionsmodellen wieder. 
Der Georegression liegt die Stationarität der Kovarianz im Raum als fundamentale Annahme zugrunde. 
Die Verteilungsmodellierung der Residuen erfolgt mittels Variogram und zugehörigen Kovariogrammen.
Im Gegensatz dazu wird im folgenden die Annahme der Stationarität ersetzt durch 
räumlich autoregressive Annahmen auf Grundlage der Gewichtsmatrix. 
% Deren spezifische Details umfasst zum einen die Konstruktion der Regressionsresiduen 
% im räumlich bezogenen Fehlermodell (spatial error model).
% Zum anderen die verzogenen Regressoren(?) im räumlich verzogenen Modell (spatial lag model)
% [SEM und SLM zu spezifisch hier, besser SAR und CAR unterscheiden]

Peter Whittle forschte ab 1949 an der Zeitreihenanalyse und übernahm eine analoge 
Theorie für stationäre Gaußprozesse auf räumliche Prozesse \cite{whittle_stationary_1954}. \cite[S. 187]{gani_craft_1986} enthält eine 
persönliche Zusammenfassung aus Whittles Sicht und seiner Zusammenarbeit mit Matern, Bartlett und weiteren bekannten
Namen der Stochastik.

\section{Spatial Statistics – Allgemeine Räumliche Autoregression}

Im folgenden werden räumliche Autoregressionsmodelle (engl. spatial autoregressive models) eingeführt. 
Wie in Kapitel \ref{ch:autocorrelation} erläutert, ist die Annahme unabhängig identisch verteilter Beobachtung aufgrund 
inhärenter räumlicher Abhängigkeiten nicht gerechtfertigt. 
Zudem muss der Einfluss räumlicher Autokorrelation getrennt werden von den impliziten Unterschieden der multivariaten Verteilung.

Im folgenden wird die relevante Menge der Raumeinheiten $\left\{ R_1,\ldots,R_n \right\}$ durch ihre 
Indizes $i=1,\ldots,n$ vereinfacht repräsentiert. Insbesondere wird es sich immer um 
die Stichprobeneinheiten handeln. Als lineares Grundmodell für Abhängigkeiten zwischen einzelnen Einheiten dient

\begin{equation} \label{eq-6.1:reg-mod-gen}
    Y_{i}=\beta_{0}+\sum_{j=1}^{k} \beta_{j} x_{ij} +\nu_{i} \, , \, i=1,\ldots,n
\end{equation}
mit $Y_i$ als relevantes Attribut für jede Raumeinheit $i$ und $(x_{ij}:j=1,\ldots ,k)$ als Menge der
Erklärungsattribute von $i$ welche den Wert $Y_{i}$ beeinflussen. Der Hauptunterschied zur Georegression ist
die Modelierung von $\nu_{i}$ über ein eigenes explizites lineares Model statt als zufälligen Fehlerterm $\varepsilon_{i}$.

Korrelierte Beobachtungen werden in Matrixschreibweise über Zusammenhang \eqref{eq-6.2:reg-mod} modelliert.

\begin{equation} \label{eq-6.2:reg-mod}
    {Y} = \mathbf{X}^{\top} \beta + \nu 
\end{equation}
%\mathds{1}  \mathds{X}
%\bbbe
%\mathbb{I}

Die Störgröße $\nu$ wird als multinomial verteilter Zufallsvektor aus (nicht notwendig normalverteilten) Fehlertermen 
mit $\bbbe[\nu]=0$ angenommen, sodass
\begin{equation*}
    \bbbe[Y]=\mathbf{X} \beta
\end{equation*}
gilt.
Während die Störterme $\nu$ im klassischen Regressionsmodell als i.i.d angenommen werden, benötigen wir nunmehr 
eine Möglichkeit, räumlich auftretende Korrelationsstrukturen in dieses Modell zu integrieren. Zum einen 
lässt sich der Fehlerprozess als \emph{raumstrukturell autoregressiver Prozess} 
(engl. spatially autoregressive process - SAR)
\begin{equation*}
    \nu = \rho \mathbf{W} \nu + \varepsilon
\end{equation*}
erfassen. Zum anderen ist eine Integration als \emph{raumstruktureller Gleitmittelprozess} (engl. spatially 
moving average process - SMA)
\begin{equation*}
    \nu = \rho \mathbf{W} \varepsilon + \varepsilon
\end{equation*}
denkbar mit Gewichtsmatrix $\mathbf{W}$, Autoregressionsparameter $\rho$ und $\varepsilon$ als 
Vektor von i.i.d.-verteilten Störgrößen. \\

Die Struktur der Korrelation zwischen Gebieten wird durch eine Varianzmatrix $V$ erfasst.
Der Trend $Y = \mathbf{X}^{T} \beta$ wird in linearer 
Abhängigkeit von unabhängigen Erklärungsvariablen $\mathbf{X}$ bzw. mehreren 
Kovariablen (engl. covariate, control variable) $X_1,\ldots,X_k$ modelliert.  

Liegt als Output keine hinreichend normale, multivariate Verteilung vor, kann eine Transformation der 
abhängigen Reaktionsvariable (auch Zielgröße oder Regressand) helfen. 

Es folgen verschiedene Ansätze von Korrelationsstrukturen, verwirklicht in den Modellklassen 
der Simultaneous Autoregressive Models (SAR) und Conditionally Autoregressive Models (CAR).

\section{Simultaneous Autoregressive Models - SAR}

Das Hauptziel ist die Modellierung der Kovarianzstruktur von $\nu$ unter Berücksichtigung der 
räumlichen Abhängigkeiten zwischen den Instanzen.
Die räumliche Gewichtsmatrix $W=[w_{ij}:i,j=1,\ldots,n]$ 
erfasst die diskrete Raumstruktur und räpresentiert oft ein Maß räumlicher Nähe. Größere Werte $w_{ij}$ 
bedeuten hierbei größere Nähe und Einfluss von $i$ und $j$ aufeinander. Jedes Residuum $\nu_{i}$ wird 
durch die Residuen der Nachbargebiete $j$ mit positiven paarweisen Raumgewichten $w_{ij}$ beeinflusst. 
Diesen Einfluss räpresentiert das lineare Teilmodell
\begin{equation} \label{eq-6.2:reg-st}
    \nu_{i}=\sum_{j=1 ; j \neq i}^{n} \alpha(w_{ij}) \nu_{j} + \varepsilon_{i}
\end{equation}   
mit $\alpha(w_{ij})$ als Einflussfunktion und $\varepsilon_{i}$ als Anteil des Residuums, welcher 
nicht durch andere Einheiten beeinflusst wird. Da die Gewichtsmatrix bereits große Flexibilität durch 
die Spezifikation der Raumstruktur bietet, wird die Funktion $\alpha$ auf die einfachste 
denkbare Form eines räumlich unabhängigen Skalierungsfaktors $\rho$ reduziert.
Folglich resultiert aus \eqref{eq-6.2:reg-st} der Zusammenhang
\begin{equation}\label{eq-6.3:reg-err}
    \nu_{i}=\rho \sum_{j = 1}^{n} w_{ij} \nu_{j} + \varepsilon_{i} \, , \quad 
    \varepsilon_{i} \sim \mathcal{N}(0,\sigma^{2}) \, , \, i= 1,\ldots,n
\end{equation}
als lineares Regressionsmodell (ohne Interzeptor) für jedes Residuum $\nu_{i}$ im Regress auf 
seine Nachbarn $\nu_{j}$ mit Koeffizienten $\rho \, w_{ij}$. Aufgrund dieser Regression der 
Residuen auf \glqq sich selbst \grqq{} wird dieses Teilmodell 
als \emph{Räumlich Autoregressives Modell der Residuen} 
(engl. spatial autoregressive model of residual dependencies) 
bezeichnet \cite{whittle_stationary_1954}, inspiriert von den Begrifflichkeiten der Zeitreihentheorie. 
Selbstregression durch individuelle Residuen wird durch Voraussetzung $w_{ii}=0$ verhindert. Alternativ kann für die 
Summation auch $i \neq j$ vorausgesetzt werden.

Nehmen wir für die intrinsischen Restkomponenten $\varepsilon_{i}$ eine \emph{i.i.d.} Verteilung gemäß 
\begin{equation} \label{eq-6.4:err-err}
    \varepsilon_{i} \sim \mathcal{N}(0,\sigma^{2}) \, , \, i=1,\ldots,n
\end{equation}  
an, so gilt $\bbbe[\varepsilon_{r}]=0$ sowie $\bbbe[\varepsilon_{r}\varepsilon_{s}]=0$ 
und $Var(\varepsilon_{r})=\sigma_{r}^{2}$. 
Parameter $\rho$ steuert den Grad der Regression. 
Für $\rho = 0$ wird jedes Residuum $e_{i}$ auf seine eigene intrinsische 
Komponente $\varepsilon_{i}$ reduziert und räumliche Abhängigkeiten verschwinden ganz. 
In diesem Fall wird Modell \eqref{eq-6.2:reg-mod} zu einem einfachen linearen Regressionsmodell reduziert.
Für große $\rho$ werden die (positiven und negativen) räumlichen 
Abhängigkeiten dominanter. Daher wird $\rho$ auch als \emph{Parameter räumlicher Abhängigkeit} 
bezeichnet. 
Außerdem muss für beliebige Paare $ij$ und $kh$ mit positiven räumlichen 
Gewichten $w_{ij},w_{kh}>0$ und einem nicht verschwindenden Parameter $\rho \neq 0$, 
\begin{equation*}
    \frac{\rho \, w_{ij}}{\rho \, w_{kh}} = \frac{w_{ij}}{w_{kh}}
\end{equation*}
gelten. Somit ist die relative Stärke ihrer räumlichen Dependenz ausschließlich durch ihre Gewichte bestimmt.
Das Modell bietet somit eine natürliche Zuordnung der Parameter, 
da $\rho$ das allgemeine Niveau räumlicher Abhängigkeit bestimmt und die räumliche 
Struktur der Gewichtsmatrix $W$ ihre relative Stärke zwischen individuellen Paaren räumlicher Einheiten.\\

Zusammen formen \eqref{eq-6.3:reg-err} und \eqref{eq-6.4:err-err} 
das \emph{Autoregressive Modell der Residuen} nach Whittle (1954)
in Matrixschreibweise, 
\begin{equation}\label{eq-6.5:reg-vec}
    \nu=\rho \mathbf{W} \nu + \varepsilon \, , \quad \varepsilon \sim \mathcal{N}(0,\sigma^{2} \mathds{I}_{n}) \, , 
    \quad \operatorname{diag}(\mathbf{W})=0
\end{equation}
Entwickelt wurde diese Form durch Ord (1975), welcher auch vom 
räumlich autoregressiven Prozess erster Ordnung sprach.
Die Modellgleichung $\nu=\rho \mathbf{W} \nu + \varepsilon$ lässt sich direkt nach $\varepsilon$ 
umstellen und ergibt somit $(\mathds{I}_{n} -\rho \mathbf{W}) \nu =\varepsilon$. 
Sofern Inverse $(\mathds{I}_{n} -\rho \mathbf{W})^{-1}$ existiert, lässt sich die 
folgende Lösung für $\nu$ in reduzierter Form erhalten
\begin{equation} \label{eq-6.6:reg-red}
    \nu=(\mathds{I}_{n} -\rho \mathbf{W})^{-1} \varepsilon ~ , \quad 
    \varepsilon \sim \mathcal{N}(0,\sigma^{2} \mathds{I}_{n}) ~ , \, \operatorname{diag}(\mathbf{W})=0
\end{equation}

\subsection{Räumlich fehlerbezogenes Modell - SEM}
Die Integration der autoregressiven Residuen aus \eqref{eq-6.5:reg-vec} in das ursprüngliche 
Regressionsmodell \eqref{eq-6.2:reg-mod} der Form $Y=\mathbf{W} \beta + \nu$ liefert nun das 
Gesamtmodell 
\begin{equation} \label{eq-6.7:sem-mod}
    Y=\mathbf{X} \beta + \nu \, , \quad \nu=\rho \mathbf{W} \nu + \varepsilon \, , \quad 
    \varepsilon \sim \mathcal{N}(0,\sigma^{2} \mathds{I}_{n}) \, , \quad 
    \operatorname{diag}(\mathbf{W})=0
\end{equation}
und wird als \emph{räumlich fehlerbezogenes Modell} 
(engl. spatial error model - SEM) mit räumlich autokorrelierten Fehlertermen bzw. 
raumstruktureller Autokorrelation im Fehlerterm bezeichnet. 
Diese Form stellt die direkte Umsetzung des Räumlich Autoregressiven Modells auf die Fehlerterme 
dar. Abweichend vom klassischen Regressionsmodell werden die Störgrößen nicht 
als i.i.d angenommen, sondern folgen einem räumlich autokorrelierten Prozess.
Zugrunde liegt die Annahme, dass alle räumlichen Abhängigkeiten in den 
unbeobachteten Fehlertermen $\nu$ liegen, woher auch der Name rührt. 
Wir gehen davon aus, mit den Regressoren in $\mathbf{X}$ nicht die gesamte 
räumliche Variabilität erklären zu können bzw. die fehlenden 
erklärenden Faktoren nicht zu kennen und daher durch $\nu$ abzudecken.

\subsubsection{Simultanitätsstruktur}

Alternativ erhalten wir mit der Umstellung $\nu=Y- \mathbf{X} \beta$ 
bzw. $\nu_{i}=Y_{i}-\sum_{j=1}^{k} \beta_{j} x_{ij}$ mit der 
Form $Y=\mathbf{X} \beta + \rho \mathbf{W} (Y-\mathbf{X} \beta) + \epsilon$ das \emph{Simultan Spezifizierte Autoregressive Modell} 
(engl. simultaneous autoregressive model -SAR). Die Bezeichnung 
der \glqq simultanen Spezifikation\grqq{} bezieht sich dabei auf die Anwendung der Modellgleichung auf 
jede Lokation $i$ gemäß
\begin{equation}
    Y_i = \beta_{0} + \sum_{j=1}^{k} 
    \beta_{j} x_{ij} + \rho \sum_{h=1}^{n} w_{ih} \left[ Y_{h}- \sum_{j=1}^{k} \beta_{j} x_{hj} \right] + \varepsilon_{i} \, , \quad
    \varepsilon_{i} \sim \mathcal{N}(0,\sigma^{2}) \, , \, i=1,\ldots,n
\end{equation}
unter simultaner Variabilität von $Y_{i}$ und $Y_{h}$. Derartige Interdependenzen sorgen für zusätzliche Komplexität 
und grenzen diese Modellklasse von den \emph{Bedingt Autoregressiven Modellen} 
(engl. conditional autoregressive models - CAR) im nächsten Abschnitt ab.

In Matrixschreibweise lässt sich das Modell in die Kurznotation
\begin{equation*}
    Y=\mathbf{X} \beta + \rho \mathbf{W} (Y-\mathbf{X} \beta) + \epsilon \quad \Rightarrow \quad  (\mathds{I}_{n}-\rho \mathbf{W}) (Y-\mathbf{X} \beta) = \varepsilon
\end{equation*}
umwandeln, wobei $(\mathds{I}_{n}-\rho \mathbf{W})$ für ein wohldefiniertes Modell regulär sein muss.
(Einige Quellen wie  \cite[S.440]{cressie_statistics_1993}, \cite[S.363]{waller_applied_2004} 
und \cite[S. 293]{bivand_applied_2013} fassen $\rho \mathbf{W} $ unter einer Matrix $ \mathbf{B}$ zusammen.)

Daraus wird die Varianz-Kovarianzmatrix von $Y$ durch
\begin{equation*}
    \Sigma_{Y}=Var(Y) = (\mathds{I}_{n}-\rho \mathbf{W})^{-1} \, \Sigma_{\varepsilon} \, (\mathds{I}_{n}-\rho \mathbf{W}^{\top})^{-1}
\end{equation*}
abgeleitet und mit $\bbbe[Y]=\mathbf{X} \beta$ unterliegt $Y$ einer multivariaten Normalverteilung. 
Oftmals wird $\Sigma_{\varepsilon}=\sigma^{2} \mathds{I}_{n}$ gesetzt und somit die Varianz-Kovarianzmatrix zu
\begin{equation*}
    \Sigma_{Y}=Var(Y) = \sigma^{2} \, (\mathds{I}_{n}-\rho W\mathbf{W}^{-1} \, (\mathds{I}_{n}-\rho \mathbf{W}^{\top})^{-1}
\end{equation*}
vereinfacht. Hierbei sind asymmetrische Gewichtsmatrizen $W$ zulässig, diese werden aber in der Praxis 
oft als symmetrisch angenommen \cite[S. 294]{bivand_applied_2013}.

Um das Modell als Instanz des allgemeinen linearen Regressionsmodells zu formulieren,
wird zunächst der Sammelterm
\begin{equation}
    B_{\rho} := \mathds{I}_{n} - \rho \mathbf{W}
\end{equation}
eingeführt und System \eqref{eq-6.6:reg-red} durch
\begin{equation}
    \nu=(\mathds{I}_{n} -\rho \mathbf{W})^{-1} \varepsilon=B_{\rho}^{-1} \varepsilon \, , \quad 
    \varepsilon \sim \mathcal{N}(0,\sigma^{2} \mathds{I}_{n}) \, , \quad \operatorname{diag}(\mathbf{W})=0    
\end{equation}
zusammengefasst.
Durch das Invarianztheorem multi-normaler Verteilungen folgt aus der 
multi-normalität von $\varepsilon$ dieselbe Verteilung für $\nu$ mit der Kovarianz
\begin{alignat*}{1}
    cov(\nu)=cov(B_{\rho}^{-1} \varepsilon) 
    & = B_{\rho}^{-1} cov(\varepsilon) (B_{\rho}^{-1})^{\top} \\
    & = B_{\rho}^{-1} (\sigma^{2} \mathds{I}_{n}) (B_{\rho}^{-1})^{\top}=
    \sigma^{2} B_{\rho}^{-1} (B_{\rho}^{\top})^{-1}=
    \sigma^{2} (B_{\rho}^{\top} B_{\rho})^{-1} =\sigma^{2} V_{\rho}
\end{alignat*}
und der räumlichen Kovarianzstruktur $V_{\rho}$, welche durch
\begin{equation}
    V_{\rho} := (B_{\rho}^{\top} B_{\rho})^{\text{-1}}
\end{equation}
alle räumlichen Aspekte der Kovarianz definiert. Hiermit wird 
SEM-Modellgleichung \eqref{eq-6.7:sem-mod} durch 
\begin{equation} \label{eq-6.8:sem-mod2}
    Y=\mathbf{X} \beta + \nu \, , \quad 
    \nu \sim \mathcal{N}(0,\sigma^{2} {V}_{\rho})
\end{equation}
umgeschrieben als allgemeines lineares Regresionsmodell.

Eine dritte Darstellungsform ensteht durch Eliminierung 
aller simultaner Beziehungen $\nu=\rho \mathbf{W} \nu + \varepsilon$ 
nach Einsetzen von $B_{\rho}$ und ergibt 
\begin{equation}
    Y=\mathbf{X} \beta + B_{\rho}^{\text{-1}} \varepsilon \, , \quad 
    \varepsilon \sim \mathcal{N}(0,\sigma^{2} \mathds{I}_{n})
\end{equation}
die reduzierte Form von Modell \eqref{eq-6.7:sem-mod}. 


\subsection{Räumlich verzogenes Modell - SLM}
Ein alternatives Modell ensteht durch die Annahme, 
dass die autoregressiven Beziehungen zwischen den abhängigen Variablen 
selbst auftreten. Es wird als \emph{räumlich verzogenes Modell} 
(engl. spatial lag model) (SLM) bezeichnet. 
Die zugrundeliegenden räumlichen Beziehungen werden auch hier 
durch die Gewichtsmatrix $\mathbf{W}$ repräsentiert. Die Grundgleichung \eqref{eq-6.1:reg-mod-gen}
wird durch Einsetzen von $\nu_{i}=\rho \sum_{h} w_{ih}Y_{h}+\varepsilon_{i}$ angepasst zu 

\begin{equation} \label{eq-6.9:slm-mod}
    Y_i = \beta_{0} + \sum_{j=1}^{k} 
    \beta_{j} x_{ij} + \rho \sum_{h=1}^{n} w_{ih}Y_{h} + \varepsilon_{i} \, , \quad
    \varepsilon_{i} \sim \mathcal{N}(0,\sigma^{2}) \, , \, i=1,\ldots,n
\end{equation}
Der autoregressive Term $\rho \sum_{h=1}^{n} w_{ij} Y_{h}$ spiegelt mögliche 
Abhängigkeiten des Wertes $Y_{i}$ von den Werten $Y_{h}$ anderer Einheiten wieder. 
Neben den Erklärungsvariablen aus $\mathbf{X}$ wird der Modelloutput auch indirekt durch die
Werte in der räumlichen Nähe beeinflusst.

\subsubsection{Simultanitätsstruktur}

Da die Residuen als unabhängig angenommen werden, scheint das obige Modell der 
gewöhnlichen Kleinste-Quadrate-Methode (KQM, engl. OLM) zu entsprechen, mit dem zusätzlichen 
Term $\rho (\sum_{h=1}^{n} w_{ih} Y_{h})$, wobei der unbekannte räumliche Abhängigkeitsparameter $\rho$ 
als ein einfacher beta-Parameter erscheint. Dies ist jedoch keineswegs der Fall, denn die $Y_{h}$ sind
Zufallsvariablen und erscheinen zudem auf beiden Seiten des Systems in der typischen \glqq Simultanitätsstruktur\grqq.
Das heißt, $Y_{i}$ wird auch in den Gleichungen für $Y_{h}$ auftreten wann immer $w_{hi}>0$ ist. Es handelt 
sich somit keineswegs um einen einfachen Term eines KQM-Models. Dies zeigt sich insbesondere in der 
Matrixnotation
\begin{equation*}
    Y=\mathbf{X} \beta +\rho \mathbf{W} Y + \varepsilon \, , \quad 
    \varepsilon \sim \mathcal{N}(0,\sigma^{2} \mathds{I}_{n})
\end{equation*}
welche wir weiter umformen.
Durch Gruppierung der Y-Terme erhalten wir
%\setlength{\abovedisplayskip}{0pt}
%\setlength{\belowdisplayskip}{0pt}
%\setlength{\abovedisplayshortskip}{0pt}
%\setlength{\belowdisplayshortskip}{0pt}
\begin{alignat*}{2}
    Y - \rho \mathbf{W} Y = \mathbf{X} \beta + \varepsilon & \Rightarrow (\mathds{I}_{n}-\rho && \mathbf{W})Y =\mathbf{X} \beta+\varepsilon \\
                                    & \Rightarrow && B_{\rho} \, Y = \mathbf{X} \beta + \varepsilon  \Rightarrow Y= B_{\rho}^{\text{-1}} \mathbf{X} \beta + B_{\rho}^{\text{-1}} \varepsilon \\
\end{alignat*}
und leiten die reduzierte Form
\begin{equation} \label{eq-6.10:slm-mod-red}
    Y=B_{\rho}^{\text{-1}} \mathbf{X} \beta + B_{\rho}^{\text{-1}} \varepsilon \, , \quad 
    \varepsilon \sim \mathcal{N}(0,\sigma^{2} \mathds{I}_{n})
\end{equation}
ab. In dieser Form ist ersichtlich, dass der räumliche Verzugsterm (engl spatial lag term) $\rho \mathbf{W} Y$ 
nicht nur ein simpler Regressionsteil ist, sondern das Modell grundlegend vom KQM abweicht.\\

Zudem kann wenn auch das Modell als Sonderform der Linearen Regression formuliert werden, wenngleich auch 
nicht so direkt wie im Fall der SEM. Der räumliche Parameter $\rho$ wird hierzu 
als bekannte Größe
behandelt und das SLM auf gegebenes $\rho$ bedingt. Hierfür betrachten wir
\begin{equation*}
    X_{\rho}=B_{\rho}^{\text{-1}} \mathbf{X}
\end{equation*}
als transformierten Datensatz. Zudem nutzen wir analog zum SEM den Sammelterm $B_{\rho}$ und 
die Kovarianzstruktur $V_{\rho}$ gemäß
\begin{equation*}
    B_{\rho}=\mathds{I}_{n}-\rho \mathbf{W} \quad \text{sowie} \quad V_{\rho} := (B_{\rho}^{\top} B_{\rho})^{\text{-1}}
\end{equation*}
um die Form
\begin{equation}
    Y=X_{\rho} \beta + u \, , \quad u \sim \mathcal{N}(0,\sigma^{2} V_{\rho})
\end{equation}
zu erreichen. Hier ist $\rho$ nicht mehr nur unbekannter Parameter in der Kovarianzmatrix $V_{\rho}$, 
sondern tritt ebenso in $X_{\rho}$ auf. Während obiger Ausdruck also die Anwendung der 
gewöhnlichen KQ-Methode auch auf räumlich verzogene Modelle (SLM) erlaubt, so ist die Anwendung eingeschränkter als 
für räumliche Fehlermodelle (SEM).

\subsubsection{Interpretation der Beta-Koeffizienten}

Ein weiterer wichtiger Unterschied zwischen SLM und SEM ist die Interpretation der Beta-Koeffizienten. 
Eine angenehme Eigenschaft der gewöhnlichen KQM ist die einfache Interpretation ihrer Beta-Koeffizienten.
Aus der gewöhnlichen KQM für die Anzahl Arbeitnehmer in der Logistik $Y_{i}$ für Gemeinde $i$ mit 
Arbeitslosigkeit als j-tem Regressor $x_{ij}$ im Modell
\begin{equation*}
    Y_{i}=\beta_{0}+\sum_{j=1}^{k} \beta_{j} x_{ij} \quad \text{mit} \, \varepsilon_{i} \sim \mathcal{N}(0,\sigma^{2})\,, \, i=1,\ldots,n
\end{equation*}
wird in einem negativen Koeffizienten $\beta_{j}$ resultieren. Die auf realisierte Daten aller Attribute einer Gemeinde bedingte Erwartung
\begin{equation} \label{eq-6.11:sem-cond-expect}
    \bbbe[Y_{i}|x_{i1},\ldots,x_{ik}]=\beta_{0}+\sum_{j=1}^{k} \beta_{j} x_{ij} \, , \quad i=1,\ldots,n
\end{equation}
gibt für $\beta_{j}$ im Mittel den (erwarteten) Abfall an Logistikarbeitnehmern der Gemeinde $i$ je 
zusätzlichem arbeitslosen Bewohner der Gemeinde an. Der bedingte Erwartungswert kann direkt für jede Gemeinde einzeln 
gebildet werden, da Attribute anderer Gemeinden hier keinen Einfuss haben.
Diese Grenzänderungen können auch als partielle Ableitungen in der Form
\begin{equation} \label{eq-6.12:sem-part-deriv}
    \frac{\partial}{\partial x_{ji}} \bbbe(Y_{i}|x_{i1},\ldots,x_{ij},\ldots,x_{ik})=\beta_{j} \, , \quad i=1,\ldots,n \, , \, j=1,\ldots,k
\end{equation}
ausgedrückt werden. Sie korrespondieren exakt zum $\beta_{j}$ Koeffizienten für Variable $x_{j}$ und erlauben eine direkte Interpretation.

Der Nachteil dieser KQM ist jedoch die gänzliche Vernachlässigung räumlicher Abhängigkeiten zwischen Gemeinden, daher wurde es zu Beginn des 
Kapitels weiterenwickelt zum SEM-Modell \eqref{eq-6.8:sem-mod2} der Form
\begin{equation*}
    Y_{i}=\beta_{0}+\sum_{j=1}^{k} \beta_{j} x_{ij} + e_{i} \quad 
    \text{mit} \, (e_{i},\ldots,e_{n}) \sim \mathcal{N}(0,V_{\rho})
\end{equation*}
mit $\bbbe[e_{i}]=0$ für alle $i$, wodurch weiterhin Gleichungen \eqref{eq-6.11:sem-cond-expect} und \eqref{eq-6.12:sem-part-deriv} gelten. 
Somit ist die Interpretation der $\beta$ Koeffizienten auf das SEM übertragbar, während räumliche Abhängigkeiten einer bestimmten Art (zwischen Residuen) integriert sind. 
Werden jedoch räumliche Abhängigkeiten zwischen den Beschäftigungszahlen $Y_{i}$ 
der Gemeinden vermutet und über ein SLM modelliert, so ist die Interpretation aufgrund der Simultanitätsstruktur 
weitaus komplexer. Dies wird ersichtlich aus der 
reduzierten Form \eqref{eq-6.10:slm-mod-red} in Verbindung mit der Wellen- bzw. Riffelzerlegung aus
Anhang A\footnote[1]{Soweit $\rho$ und W Konvergenzbedingung $ \left| \rho \right| <1/\lambda_{W}$ erfüllen}.

\begin{alignat*}{1}
    \bbbe[Y|X]=B_{\rho}^{-1} X \beta = (\mathds{I}_{n} - \rho W)^{-1} X \beta & = 
    (\mathds{I}_{n} + \rho W + \rho^{2} W^{2} + \ldots) X \beta \\ & =
    X \beta + \rho W X \beta + \rho^{2} W^{2} X \beta + \ldots
\end{alignat*}

Um hierauf partielle Ableitungen anzuwenden, müssen zuerst \emph{alle} Attribute für \emph{alle} Gemeinden 
spezifiziert werden. Eine getrennte Darstellung für $Y_{i}$ muss im Detail entwickelt werden.
Da nun Interaktionen zwischen einzelnen Gemeinden modelliert werden, sind für jede Gemeinde $i$ 
die partiellen Ableitungen ${\partial} \big/ {\partial x_{lj}}$ bezüglich weiterer Gemeinden ($l \neq i$) und Attribute $j$ 
von $\bbbe [Y_{i}|x_{1i},\ldots,x_{ji},\ldots,x_{ki}] $ relevant und müssen im Regressionskoeffizienten berücksichtigt werden. 
Zuvor traten im SEM keine solchen Effekte zwischen Gemeinden auf und diese partiellen Ableitungen verschwanden für $l \neq i$ einfach.
Im SLM jedoch resultiert eine Erhöhung von $x_{ij}$, z.B. der Arbeitslosigkeit in Gemeinde $i$, nicht nur direkt in einer Verringerung 
der Logistikbeschäftigten innerhalb der gleichen Gemeinde $i$, sondern sorgt auch indirekt für Beschäftigungsveränderungen (positiv wie negativ) 
in allen anderen Gemeinden $l \neq i$. Im Umkehrschluss sorgen diese Änderungen in $l$ wiederum zu indirekten Rückkopplungseffekten in $i$. 
Diese räumlichen Riffel- bzw. Welleneffekte führen zu komplexen Zwischenabhängigkeiten, welche die einzelnen Beta-Koeffizienten beeinflussen.
Zur Trennung dieser Effekte zerlegen wir die Matrixprodukte der reduzierten Form in Summationen der einzelnen Vektoroperationen. 
Zur besseren Übersicht nutzen wir kurzzeitig für beliebige $n \times m$ Matrizen 
$A=[a_{ij}:i=1,\ldots,n \, , \, j=1,\ldots,m]$ die Notation $A(i,j)=a_{ij}$ für einzelne Matrixelemente 
und $A(\cdot,j)$ für die j-te Spalte in $A$. Zudem kürzen wir $C := B_{\rho}^{-1}$ ab.

\begin{alignat*}{1}
    \bbbe[Y|X] & = C X \beta = C \sum_{j=1}^{k} \beta_{j} X(\cdot,j) \\
    & = \sum_{j=1}^{k} \beta_{j} \left[ \sum_{h=1}^{n} X(h,j) C(\cdot,h) \right] 
    = \sum_{h=1}^{n} \sum_{j=1}^{k}  X(h,j) \, \beta_{j} \, C(\cdot,h)
\end{alignat*}

In dieser Doppelsumme stellt die innere Summe für ein festes $h$ eine Linearkombination 
aus Spaltenvektoren dar. Die äußere Summe wendet darüber eine zweite Linearkombination 
für alle $h$ an und ergibt einen finalen Spaltenvektor an Y-Werten. Somit lässt sich eine 
einzelne Zeile $i$ aus $ \bbbe[Y|X]$ schreiben als
\begin{equation*}
    \bbbe[Y_{i}|X] = \sum_{h=1}^{n} \sum_{j=1}^{k}  X(h,j) \, \beta_{j} \, C(i,h)
\end{equation*}
Hiervon kann nun direkt die partielle Ableitung
\begin{equation*}
    \frac{\partial}{\partial x_{ij}} \bbbe[Y_{i}|X] = \beta_{j} C(i,i)
\end{equation*}
gebildet werden. Im Gegensatz zur partiellen SEM-Ableitung \eqref{eq-6.12:sem-part-deriv} hängt 
dieser Grenzeffekt nicht nur von $\beta_{j}$ ab, sondern 
auch vom i-ten Diagonaleintrag der Matrix $C$ bzw. $B_{\rho}^{-1}$, welcher explizit die Form
\begin{equation*}
    B_{\rho}^{-1}(i,i) = 1 + \rho W(i,i) + \rho^{2} W^{2}(i,i) + \ldots = 1 + \rho^{2} W^{2}(i,i)
\end{equation*}
annimmt, da die Diagonaleinträge $W(i,i)$ auf Null gesetzt werden. Andererseits 
sind $\rho^{2} W^{2}(i,i)$ und höhere Ordnungen positiv, und der Effekt jedes $\beta_{j}$ wird 
durch diese räumlichen Effekte aufgebläht, wie oben beschrieben. 
Da nun $Y_{i}$ auch durch Änderungen in $Y_{h}$ beeinflusst wird,folgt für 
Attribut $j$ in Gemeinde $h$ die Grenzänderung
\begin{equation*}
    \frac{\partial}{\partial x_{hj}} \bbbe[Y_{i}|X] = \beta_{j} C(i,h)
\end{equation*}
auf Erwartungswert $\bbbe[Y_{i}|X]$. Der Gesamteffekt aller Gemeinden auf $\bbbe[Y_{i}|X]$ 
durch Attribute aus anderen Gemeinden wird als \emph{indirekte Effekte} bezeichnet. 
Analog wird der Gesamteffekt aller Attribute in der gleichen 
Raumeinheit bzw. Gemeinde $i$ (mittels totalem Differential?) mit \emph{direkten Effekten} benannt.
\cite[S.335]{waller_applied_2004}
%LeSage Pace 2009 Section 2.7.1

%Waller-Gotway S.335
%Greene S.297

\subsection{Kombiniertes Modell}
Während der Entwicklung des SL-Modells stellt sich die Frage, warum alle unbeobachteten Faktoren 
als räumlich unabhängig behandelt werden. Es ist praktisch sehr wohl möglich sowohl zwischen den Y-Variablen 
als auch den Residuen $\varepsilon$ räumlich autoregressive Abhängigkeiten zu messen. Wird nun zur Unterscheidung 
mit $M$ eine eigene Gewichtsmatrix und $\lambda$ ein zusätzlicher Abhängigkeitsparameter für die Fehlerkomponente 
eingeführt, so lassen sich beide Modelle gemäß
\begin{equation*}
    Y=\rho W Y + X \beta + \nu \, , \, \nu=\lambda M \nu + \varepsilon \, , \quad 
    \varepsilon \sim \mathcal{N}(0,\sigma^{2} \mathds{I}_{n})
\end{equation*}
kombinieren und in die zugehörige reduzierte Form
\begin{alignat*}{1}
    &(\mathds{I}_{n} - \rho W) Y = X \beta (\mathds{I}_{n} - \lambda M)^{-1} \varepsilon \\
    \Rightarrow \, & Y=(\mathds{I}_{n}-\rho W)^{-1} X \beta + 
    (\mathds{I}_{n}-\rho W)^{-1}(\mathds{I}_{n}-\lambda M)^{-1} \varepsilon
\end{alignat*}
bringen. Dieses Modell wurde durch Kelejian and Prucha (2010) als Räumlich Autoregressives Modell mit 
autoregressiven Störungen
(engl. Spatial Autoregressive Model with Autoregressive disturbances) bezeichnet und mit SARAR(1,1) abgekürzt.
In unserer Arbeit wird dieses Modell zur Konstruktion von Vergleichstests 
zwischen SLM und SEM als Instanzen derselben Modellklasse dienen. Daher beschränken wir uns auf 
den Spezialfall $M=W$ und erhalten
\begin{equation} \label{eq-6.20:comb-mod}
    Y=\rho W Y + X \beta + \nu \, , \, \nu=\lambda W \nu + \varepsilon \, , \quad 
    \varepsilon \sim \mathcal{N}(0,\sigma^{2} \mathds{I}_{n})
\end{equation}
als Kombiniertes Modell in der reduzierten Form: 
\begin{equation*}
    Y=(\mathds{I}_{n}-\rho W)^{-1} X \beta + (\mathds{I}_{n}-\rho W)^{-1}(\mathds{I}_{n}-\lambda W)^{-1} \varepsilon
\end{equation*}
Somit resultiert für gegebene Gewichtsmatrix W aus Kombinationsmodell \eqref{eq-6.20:comb-mod} 
mit $\rho=0$ das SE-Modell und mit $\lambda=0$ das SL-Modell. Zwar ist dieses Modell wohldefiniert und kann 
prinzipell zur simultanen Schätzung sowohl von $\rho$ als auch $\lambda$ herangezogen werden. 
Jedoch können die Ergebnisse instabil sein, da beide Parameter auf die gleiche Matrix $W$ bezogen 
ähnliche Rollen spielen.

\section{Conditional Autoregressive Models - CAR}

Während dieses Modell ein zum SEM ähnliches Konzept aufweist, liegt aus statistischer 
Sicht ein fundamentaler Unterschied vor. Statt in unserem Beispiel die gemeinsame 
Verteilung $(Y_{1},\ldots,Y_{n})$ der Arbeitnehmer aller Gemeinden zu modellieren, ist 
dieser Ansatz ausgerichtet auf die einzelnen bedingten Verteilungen einer jeden 
Beschäftigungszahl $Y_{i}$, gegeben alle anderen. Der Vorteil ist die Vermeidung einer 
komplexen Simultanitätsstruktur. Alle univariaten bedingten Verteilungen, welche von 
multinomial verteilten Zufallsgrößen abgeleitet werden, wiederum selbst normal.


Die reduzierte Form der SE-Modellgleichung wird angepasst
\begin{alignat*}{1}
    Y=X \beta + B_{\rho}^{\text{-1}} \varepsilon & \Rightarrow Y - X \beta = B_{\rho}^{\text{-1}} \varepsilon 
    \Rightarrow B_{\rho} (Y-X \beta) = \varepsilon \\
    & \Rightarrow  (\mathds{I}_{n} - \rho W)(Y-X \beta) = \varepsilon \\
    & \Rightarrow  Y - X \beta - \rho W (Y-X \beta) = \varepsilon \\
    & \Rightarrow  Y = X \beta - \rho W (Y-X \beta) + \varepsilon \\
\end{alignat*}


\section{Parameterschätzung}
Wir nutzen die bekannt \emph{Methode größter Plausibilität} (engl. Maximum-Likelihood-Methode - ML).
Für gegebene Stichprobenbeobachtung $y$ und Parametervektor $\theta$ aus $ \Theta \subseteq \mathds{R}$ ist 
der ML-Schätzer durch
\begin{equation*}
    f(y|\hat{\theta})=\max_{\theta} f(y|\theta)
\end{equation*}
definiert. Die korrespondierende Likelihoodfunktion $l(\cdot|y)$ wird 
\begin{equation*}
    l(\theta|y) := f(y|\theta)
\end{equation*}
verdeutlicht die Abhängigkeit vom unbekannten Parameter $\theta$ für bekannte $y$.
Da Wahrscheinlichkeitsdichten nichtnegative Werte annehmen und die Log-Likelihoodfunktion 
$L(\theta|y) := \operatorname{log} \left[ l(\theta|y) \right]$ stets monoton wachsend bezüglich $l(\theta|y)$ 
ist, kann der ML-Schätzer $\hat{\theta}$ auch durch die log-Likelihoodbedingungn
\begin{equation*}
    L(\hat{\theta}|y)=\max_{\theta} L(\theta|y) =\max_{\theta} \operatorname{log} (f(y|\theta))
\end{equation*}
beschrieben werden.

\subsection{ML-Schätzung im klassischen Regressionsmodell}

Wir gehen vom KLN-Modell mit normalverteilten Störgrößen aus und und sichern somit eine multivariate Normalverteilung 
der Zielgröße $Y \sim \mathcal{N}(\mathbf{X} \beta ,\sigma^2 \mathds{I}_{n})$. Wir nutzen
\begin{equation*}
    \bullet ~ \frac{1}{\sqrt{\det(\sigma^2 \mathds{I}_{n})} }= \frac{1}{\sqrt{(\sigma^{2})^n \det(\mathds{I}_{n})}}  = \frac{1}{\sqrt{(\sigma^{2})^n} } \\
    \quad \text{sowie} \quad \bullet ~ (\sigma^2 \mathds{I}_{n})^{-1} = \frac{1}{\sigma^2} \mathds{I}_{n}^{-1} = \frac{\mathds{I}_{n}}{\sigma^2}
\end{equation*}
und integrieren diese Vereinfachungen für gegebene Stichprobe $\mathbf{y}$ und gesuchten
Parametervektor $\theta=(\beta_{0},\beta_{1},\ldots,\beta_{k},\sigma^{2})$ mit 
$\Sigma=\sigma^2 \mathds{I}_{n}$ in die Dichtefunktion
\begin{alignat*}{2}
    f(\mathbf{y}|\beta,\sigma^2) & = \frac{1}{ \sqrt{(2 \pi)^{n} \det \mathbf{\Sigma} } } 
    &&\exp \left( -\frac{1}{2 } \, \left(\mathbf{y-X\beta} \right)^{\top} \mathbf{\Sigma}^{-1} \left( \mathbf{y-X\beta} \right) \right)\\
    & =\frac{1}{ \sqrt{(2 \pi  \sigma^2)^n } } 
    &&\exp \left(-\frac{1}{2 \sigma^2} \left( \mathbf{y-X\beta} \right)^{\top} \left( \mathbf{y-X\beta} \right) \right)
\end{alignat*}
der Zielverteilung. Daraus folgt die log-Likelihoodfunktion der GKQ-Methode durch
\begin{equation*}
    \operatorname{L}(\theta|\mathbf{y})=\log \left( f(\mathbf{y}|\theta) \right)= -\frac{n}{2} \operatorname{log}(2\pi) -\frac{n}{2} \operatorname{log}(\sigma^{2})
    -\frac{1}{2\sigma^{2}} \left( \mathbf{y-X\beta} \right)^{\top} \left( \mathbf{y-X\beta} \right)
\end{equation*}
Zur Ermittlung der ML-Schätzer $\hat{\theta}=(\hat{\beta},\hat{\sigma}^2)$ erfolgt die Maximierung von XX. 
Für beliebige $\sigma^2$ wird $\operatorname{L}$ bezüglich $\beta$ maximiert 
durch Minimieren des letzten Terms, auch als \emph{mittlerer quadratischer Fehler} ($\mathbf{MQF}$)
\begin{equation} \label{eq-6.21:mqf-beta}
    \operatorname{MQF}(\beta):=\left( \mathbf{y-X\beta} \right)^{\top} \left( \mathbf{y-X\beta} \right)=
    \mathbf{y}^{\top} \mathbf{y} -2 \mathbf{y}^{\top} \mathbf{X} \beta + \beta^{\top} \mathbf{X}^{\top} \mathbf{X} \beta
\end{equation}
bezeichnet. Diese quadratische Form in der Unbekannten $\beta$ liefert für das Minimierungsproblen das 
Optimalitätskriterium erster Ordnung in der Form
\begin{equation*}
    0 \overset{!}{=} \nabla \operatorname{MQF}(\beta)= -2 X^{\top} y + 2 X^{\top} X \beta \Rightarrow X^{\top} X \beta=X^{\top} y
\end{equation*}
und nutzt die Symmetrie der quadratischen Matrix $X^{\top}X$. 
Hat $X$ den vollen Rang $k+1$, d.h. es treten keine Kolinearitäten zwischen Spalten auf, 
so ist $X^{\top}X$ regulär. Somit ergibt
\begin{equation}
    \hat{\beta} = (X^{\top}X)^{-1} X^{\top} Y
\end{equation}
die eindeutige Lösung für den GKQ-Schätzer von $\beta$ zu Stichprobenrealisierungen $y$ von $Y$. 
Die Hesse-Matrix
\begin{equation*}
    H_{MQF}(\beta)=\nabla (-2 X^{\top} y + 2 X^{\top} X \beta) = 2 X^{\top} X
\end{equation*}
liefert durch positive Definitheit der Matrix $X^{\top} X$ für reguläre $X$ das
Optimalitätskriterium zweiter Ordnung des Minimierers $\hat{\beta}$.
(BEWEIS?-Für Matrix mit vollem Rang folgt positive Definitheit der quadratischen Form)
Der Stichprobenumfang $n$ muss hierbei deutlich über der Anzahl Parameter $k+1$ liegen.
Eine Verzerrung (engl. bias) des Schätzers $\hat{\beta}$ 
\begin{equation*}
    \bbbe[\hat{\beta}]=\bbbe[(X^{\top}X)^{-1} X^{\top} Y] = 
    (X^{\top}X)^{-1} X^{\top} \bbbe[Y] = (X^{\top}X)^{-1} X^{\top} X \beta = \beta
\end{equation*}
liegt nicht vor. Diese Eigenschaft der Unverzerrtheit ist 
insbesondere unabhängig von $\operatorname{cov(\varepsilon)}$. Es ist nur die korrekte 
Spezifikation des linearen Trendes $X \beta$ über $\bbbe[\varepsilon]=0$ notwendig.

Die GKQ-Methode ist nicht direkt erweiterbar auf $\sigma^2$. Statt dessen 
wird der Minimierer $\hat{\beta}$ der MQF aus Gleichung \eqref{eq-6.21:mqf-beta} substituiert 
um die reduzierte Funktion
\begin{equation*}
    L_c(\sigma^{2}|y) \equiv L(\hat{\beta},\sigma^{2}|y)=-\frac{n}{2} \operatorname{log}(2\pi) 
    -\frac{n}{2} \operatorname{log}(\sigma^{2})
    -\frac{1}{2\sigma^{2}} \left( y-X \hat{\beta} \right)^{\top} \left( y-X \hat{\beta} \right)
\end{equation*}
abzuleiten. Diese wird als konzentrierte Likelihood-Funktion von $\sigma^2$ bezeichnet. 
Maximieren dieser Funktion bezüglich Parameter $\sigma^2$ liefert 
\begin{align*}
    0 \overset{!}{=} \frac{d}{d \, \sigma^2} L_c(\sigma^{2}|y) &= -\frac{n}{2} \left(\frac{1}{\sigma^{2}}\right) 
    + \frac{1}{2} \left(\frac{1}{(\sigma^{2})^2}\right) \left( y-X \hat{\beta} \right)^{\top} \left( y-X \hat{\beta} \right)\\
    &=-n + \frac{1}{\sigma^{2}} \left( y-X \hat{\beta} \right)^{\top} \left( y-X \hat{\beta} \right)
\end{align*}
und schließlich den exakten ML-Schätzer 
\begin{equation*}
    \hat{\sigma}^{2}=\frac{1}{n} \left( y-X \hat{\beta} \right)^{\top} \left( y-X \hat{\beta} \right)
\end{equation*}
Der Maximierer $\hat{\sigma}^{2}$ muss zudem beim Einsetzen in die zweite 
Ableitung von $L_c$ negative Werte liefern.
Der ML-Schätzer von $\sigma^2=\operatorname{var}(\varepsilon)=\bbbe[\varepsilon^2]$ lässt sich zudem über 
die Residuen ausdrücken
\begin{equation*}
    \hat{\sigma}^{2}= \frac{1}{n} \hat{\varepsilon}^{\top} \hat{\varepsilon}=\frac{1}{n} \sum_{i=1}^{n} \hat{\varepsilon}_{i}^2
\end{equation*}

\subsection{ML-Schätzung im verallgemeinerten Regressionsmodell}

Im Anschluss erfolgt die Verallgemeinerung auf das VLN-Modell mit normalverteilten Störgrößen 
und resultiert somit in der verallgemeinerten multivariaten Normalverteilung 
der Zielgröße $Y \sim \mathcal{N}(\mathbf{X\beta} ,\sigma^2 \mathbf{V})$. Wir nutzen
\begin{equation*}
    \bullet ~ \frac{1}{\sqrt{\det(\sigma^2 \mathbf{V})} }= \frac{1}{\sqrt{(\sigma^{2})^n \det \mathbf{V} }} \\
    \qquad \text{sowie} \qquad \bullet ~ (\sigma^2 \mathbf{V})^{-1}  = \frac{\mathbf{V}^{-1}}{\sigma^2}
\end{equation*}
und erstellen erneut für gegebene Stichprobe $\mathbf{y}$ und
Parametervektor $\theta=(\beta_{0},\beta_{1},\ldots,\beta_{k},\sigma^{2})$ mit 
$\Sigma=\sigma^2 \mathbf{V}$ in die Dichtefunktion
\begin{alignat*}{2}
    f(\mathbf{y}|\beta,\sigma^2) & = \frac{1}{ \sqrt{(2 \pi)^{n} \det \mathbf{\Sigma} } } 
    &&\exp \left( -\frac{1}{2 } \, \left(\mathbf{y-X\beta} \right)^{\top} \mathbf{\Sigma}^{-1} \left( \mathbf{y-X\beta} \right) \right)\\
    & =\frac{1}{ \sqrt{(2 \pi  \sigma^2)^n \det \mathbf{V} } } 
    &&\exp \left(-\frac{1}{2 \sigma^2} \left( \mathbf{y-X\beta} \right)^{\top} \mathbf{V}^{-1} \left( \mathbf{y-X\beta} \right) \right)
\end{alignat*}
der Zielverteilung. Daraus folgt die log-Likelihoodfunktion der VKQ-Methode durch
\begin{equation*}
    \operatorname{L}(\theta|\mathbf{y})= -\frac{n}{2} \log(2\pi) -\frac{n}{2} \log(\sigma^{2}) -\frac{1}{2} \log(\det \mathbf{V} )
    -\frac{1}{2\sigma^{2}} \left( \mathbf{y-X\beta} \right)^{\top} \mathbf{V}^{-1} \left( \mathbf{y-X\beta} \right)
\end{equation*}
In Erweiterung zur klassischen Regression wird  $\hat{\beta}$ für beliebige $\sigma^2$ die Funktion $\operatorname{L}(\beta,\sigma^2|\mathbf{y})$ 
maximieren, indem deren quadratischer Anteil $\left( \mathbf{y-X\beta} \right)^{\top} \mathbf{V}^{-1} \left( \mathbf{y-X\beta} \right)$ minimiert wird.
Diese quadratische Form lässt sich als gewichtetes KQ-Problem interpretieren. Die Cholesky-Zerlegung für $\mathbf{V}$ 
ist durch das Cholesky-Theorem gesichert und
\begin{equation*}
    \mathbf{V}=\mathbf{T} \mathbf{T}^{\top} ~ \Rightarrow ~ \mathbf{V}^{-1} 
    = \left( \mathbf{T}^{\top} \right)^{-1} \mathbf{T}^{-1}
    = \left( \mathbf{T}^{-1} \right)^{\top} \mathbf{T}^{-1}
\end{equation*}
erlaubt es, den quadratischen Term
\begin{align*}
    \left( \mathbf{y-X\beta} \right)^{\top} \mathbf{V}^{-1} \left( \mathbf{y-X\beta} \right) 
    &= \left( \mathbf{y-X\beta} \right)^{\top} (\mathbf{T}^{-1})^{\top} \mathbf{T}^{-1} \left( \mathbf{y-X\beta} \right) \\ 
    &= \left( \mathbf{T}^{-1} \mathbf{y}- \mathbf{T}^{-1} \mathbf{X\beta} \right)^{\top} \left( \mathbf{T}^{-1} \mathbf{y}- \mathbf{T}^{-1} \mathbf{X\beta} \right) \\
    &= \left( \mathbf{\tilde{y}-\tilde{X} \beta} \right)^{\top} \left( \mathbf{\tilde{y}-\tilde{X} \beta} \right)
\end{align*}
in eine reduzierte Form zu bringen. Diese entspricht der klassischen Version unter Transformationen 
$\mathbf{T}^{-1} \mathbf{y}$ und $\mathbf{T}^{-1} \mathbf{X}$. 
Daraus ergibt sich der ML-Schätzer des KLN Modells
\begin{align*}
    \hat{\beta}=\left( \mathbf{\tilde{X}}^{\top} \mathbf{\tilde{X}} \right)^{-1} \mathbf{\tilde{X}}^{\top} \mathbf{\tilde{y}} 
    &=\left[ \left( \mathbf{T}^{-1} \mathbf{X} \right)^{\top} \left( \mathbf{T}^{-1} \mathbf{X} \right) \right]^{-1}
    \left( \mathbf{T}^{-1} \mathbf{X} \right)^{\top} \left( \mathbf{T}^{-1} \mathbf{y} \right)\\
    &=\left[ \mathbf{X}^{\top} \left(\mathbf{T}^{\top} \right)^{-1} \mathbf{T}^{-1} \mathbf{X} \right]^{-1}
    \mathbf{X}^{\top} \left(\mathbf{T}^{\top} \right)^{-1} \mathbf{T}^{-1} \mathbf{y}
    =\left[ \mathbf{X}^{\top} \mathbf{V}^{-1} \mathbf{X} \right]^{-1}
    \mathbf{X}^{\top} \mathbf{V}^{-1} \mathbf{y}
\end{align*}
unter Rücktransformation der Cholesky-Zerlegung. 
Für den ML-Schätzer $\hat{\sigma}^2$ finden wir analog zur KLN  
\begin{align*}
    \hat{\sigma}^{2} = \frac{1}{n} \left( \mathbf{\tilde{y}-\tilde{X} \hat{\beta}} \right)^{\top} \left( \mathbf{\tilde{y}-\tilde{X} \hat{\beta}} \right) 
    &= \frac{1}{n} \left( \mathbf{T}^{-1} \mathbf{y} - \mathbf{T}^{-1} \mathbf{X} \hat{\beta} \right)^{\top} \left( \mathbf{T}^{-1} \mathbf{y}- \mathbf{T}^{-1} \mathbf{X} \hat{\beta} \right)\\
    &= \frac{1}{n} \left( \mathbf{y} - \mathbf{X} \hat{\beta} \right)^{\top} \left( \mathbf{T}^{-1} \right)^{\top} \left( \mathbf{T}^{-1} \right) \left( \mathbf{y}- \mathbf{X} \hat{\beta} \right)\\
    &= \frac{1}{n} \left( \mathbf{y} - \mathbf{X} \hat{\beta} \right)^{\top} \qquad \mathbf{V}^{-1} \quad \left( \mathbf{y}- \mathbf{X} \hat{\beta} \right)
\end{align*}
und interpretieren die ML-Schätzung aus VLN-Modellen als direkte Erweiterung aus der KLN-Modelierung. 

\subsection{ML-Schätzung für SE-Modelle}
Bezüglich einer Gewichtsmatrix $\mathbf{W}$ ist die Matrix $\mathbf{B}_{\rho}=(\mathds{I}_{n} - \rho \mathbf{W})^{-1}$ gegeben und 
definiert die räumliche Kovarianzstruktur $\mathbf{V}_{\rho}=\left(\mathbf{B}_{\rho}^{\top} \mathbf{B}_{\rho} \right)^{-1} 
=\mathbf{B}_{\rho}^{-1} \left(\mathbf{B}_{\rho}^{-1}\right)^{\top}$ aus Gleichung XX. 
Somit ist das SE-Modell ein Spezialfall der VLN-Modellform mit $\mathbf{V}_{\rho}$ als Sonderform von $\mathbf{V}$. 
In $\mathbf{V_{\rho}}$ ist im Gegensatz zu $\mathbf{V}$ jetzt $\rho$ als zusätzlicher Parameter enthalten. Der Parametervektor wird erweitert zu 
$\theta=(\beta_{0},\beta_{1},\ldots,\beta_{k},\sigma^{2},\rho)$ und mit
\begin{equation*}
    \operatorname{L}(\theta|\mathbf{y})= -\frac{n}{2} \log(2\pi) -\frac{n}{2} \log(\sigma^{2}) -\frac{1}{2} \log(\det \mathbf{V_{\rho}} )
    -\frac{1}{2\sigma^{2}} \left( \mathbf{y-X\beta} \right)^{\top} \mathbf{V_{\rho}}^{-1} \left( \mathbf{y-X\beta} \right)
\end{equation*}
resultiert daraus die log-Likelihoodfunktion. Bedingt auf ein gegebenes $\rho$ folgen erneut wie im VLN-Modell 
die ML-Schätzer 
\begin{equation*}
    \hat{\beta}_{\rho} = \left( \mathbf{X}^{\top} \mathbf{V}_{\rho}^{-1} \mathbf{X} \right)^{-1}
    \mathbf{X}^{\top} \mathbf{V}_{\rho}^{-1} \mathbf{y} 
    \qquad \text{sowie} \qquad
    \hat{\sigma}_{\rho}^{2} = \frac{1}{n} \left( \mathbf{y} - \mathbf{X} \hat{\beta}_{\rho} \right)^{\top} 
    \mathbf{V}_{\rho}^{-1} \left( \mathbf{y}- \mathbf{X} \hat{\beta}_{\rho} \right)
\end{equation*}
mit Index $\rho$ als Audruck der parametrischen Abhängigkeit. 
Die explizite Form der Schätzer als geschlossen darstellbare Funktion in Abhängigkeit von $\rho$ wird in 
die Likelihoodfunktion eingesetzt um mit 
$ \operatorname{L_{c}}(\rho|\mathbf{y}) \equiv \operatorname{L}(\hat{\beta}_{\rho},\hat{\sigma}_{\rho}^{2},\rho|\mathbf{y}) $ 
die \emph{konzentrierte Log-Likelihoodfunktion} 
\begin{equation*}
    \operatorname{L_{c}}(\rho|\mathbf{y})= -\frac{n}{2} \log(2\pi) -\frac{n}{2} \log(\hat{\sigma}_{\rho}^{2}) -\frac{1}{2} \log(\det \mathbf{V_{\rho}} )
    -\frac{1}{2 \hat{\sigma}_{\rho}^{2}} \left( \mathbf{y-X \hat{\beta}_{\rho}} \right)^{\top} \mathbf{V_{\rho}}^{-1} \left( \mathbf{y-X \hat{\beta}_{\rho}} \right)
\end{equation*}
für festes $\rho$ zu erhalten, wie es zuvor in der KLN für $\sigma^2$ angewendet wurde. Hiervon lässt sich der Schlussterm durch
\begin{equation*}
    -\frac{1}{2 \hat{\sigma}_{\rho}^{2}} \left( \mathbf{y-X \hat{\beta}_{\rho}} \right)^{\top} \mathbf{V_{\rho}}^{-1} \left( \mathbf{y-X \hat{\beta}_{\rho}} \right)
    = -\frac{1}{2 \hat{\sigma}_{\rho}^{2}} \left[ n \, \hat{\sigma}_{\rho}^{2} \right] = -\frac{n}{2}
\end{equation*}
auf eine Konstante vereinfachen. Zur weiteren Reduktion wird unter Anwendung der Eigenschaften 
von Determinaten und Inversen die Form
\begin{align*}
    \det \mathbf{V_{\rho}} = \det \left(  \left(\mathbf{B}_{\rho}^{\top} \mathbf{B}_{\rho} \right)^{-1} \right) 
    & = \det \left(\mathbf{B}_{\rho}^{-1} \right)  \det \left(\left( \mathbf{B}_{\rho}^{\top} \right)^{-1} \right) \\
    & = \left( \det \mathbf{B}_{\rho} \right)^{-1}  \left( \det \mathbf{B}_{\rho}^{\top} \right)^{-1} 
    = \left( \det \mathbf{B}_{\rho} \right)^{-2}
\end{align*}
ausgenutzt. Beide Vereinfachungen bringen uns schließlich mit
\begin{equation*}
    \operatorname{L_{c}}(\rho|\mathbf{y})= -\frac{n}{2} \left[ 1+ \log(2\pi)+ \log(\hat{\sigma}_{\rho}^{2}) \right] 
    + \log(\det \mathbf{B}_{\rho} )
\end{equation*}
die vereinfachte konzentrierte Log-Likelihoodfunktion für $\rho$. 
Da $\operatorname{L_{c}}(\rho|\mathbf{y})$ eine 
hinreichend glatte Funktion in einer Variablen darstellt, lässt sich diese Funktion 
mittels Liniensuchverfahren maximieren, um Schätzer $\hat{\rho}$ zu ermitteln. Dieser wird dann in die 
Gleichung für $\hat{\beta}_{\rho}$ und für $\hat{\sigma}_{\rho}^{2}$ eingesetzt. Für 
große Stichprobengrößen $n$ wird eine Spektralzerlegung zur Berechnung von $\det \mathbf{B}_{\rho}$ genutzt. \cite[S. 300]{bivand_applied_2013}
Für zusätzliche Details wird auf \cite[S.440]{cressie_statistics_1993} verwiesen.

%\subsection{ML-Schätzung für SL-Modelle}%

%Waller Gotway 2004 S.363  - 379
%Cressie 1993 S.440    -  228

 